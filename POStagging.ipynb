{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"pos_tagging_tutorial.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"6X0UlOy_xdqo","colab_type":"text"},"source":["# Parts of Speech Tagging Tutorial\n","\n","The purpose of this tutorial is to experiment with Part-of-Speech tagging, using the tools provided by NLTK. This tutorial is taken from [here](https://www.cs.bgu.ac.il/~elhadad/nlp18/NLTKPOSTagging.html) as part of an NLP course.\n","\n","We will make use of the contents of [Chapter 5](http://www.nltk.org/book/ch05.html) of the \n","[Natural Language Processing with Python --- Analyzing Text with the Natural Language Toolkit](http://www.nltk.org/book). As experimental dataset, we will use the [Brown Corpus](http://en.wikipedia.org/wiki/Brown_Corpus). The Brown Corpus defines a tagset (specific collection of part-of-speech labels) that has been reused in many other annotated resources in English. The [universal tagset](http://universaldependencies.org/u/pos/) includes 17 tags:\n","\n","Tag\t| Meaning\t | Examples\n","----|------------|----------\n","ADJ\t| adjective\t | new, good, high, special, big, local\n","ADV\t| adverb\t | really, already, still, early, now\n","CONJ| conjunction| and, or, but, if, while, although\n","DET\t| determiner | the, a, some, most, every, no\n","X\t| other, foreign words | dolce, ersatz, esprit, quo, maitre\n","NOUN | noun\t     | year, home, costs, time, education\n","PROPN| proper noun | Alison, Africa, April, Washington\n","NUM\t | numeral\t| twenty-four, fourth, 1991, 14:24\n","PRON | pronoun\t| he, their, her, its, my, I, us\n","ADP  | adposition, preposition | on, of, at, with, by, into, under\n","AUX\t | auxiliary verb | has (done), is (doing), will (do), should (do), must (do), can (do)\n","INTJ | interjection | ah, bang, ha, whee, hmpf, oops\n","VERB | verb | is, has, get, do, make, see, run\n","PART | particle | possessive marker 's, negation 'not'\n","SCONJ | subordinating conjunction: complementizer, adverbial clause introducer | I believe 'that' he will come, if, while\n","SYM\t| symbol | $, (C), +, *, /, =, :), john.doe@example.com\n","\n","\n","\n","Note that the decision on how to tag a word, without more information is ambiguous for multiple reasons:\n","\n","- The same string can be understood as a `noun` or a `verb` (e.g, **book**).\n","- Some POS tags have a systematically ambiguous definition: a present participle can be used in progressive verb usages (I am going:VERB), but it can also be used in an adjectival position modifying a noun: (A striking:ADJ comparison). In other words, it is unclear in the definition itself of the tag whether the tag refers to a syntactic function or to a morphological property of the word.\n"]},{"cell_type":"markdown","metadata":{"id":"HyKAE15gxdqq","colab_type":"text"},"source":["## 0. Working on the Brown Corpus with NLTK\n","\n","NLTK contains a collection of tagged corpora, arranged as convenient Python objects. We will use the **Brown corpus** in this experiment:\n","\n","* The `tagged_sents` version of the corpus is a list of sentences. Each sentence is a list of tuples `(word, tag)`. \n","\n","* With `tagged_words`, one can access the corpus as a flat list of tagged words."]},{"cell_type":"code","metadata":{"id":"4l-ZGAi6xdqs","colab_type":"code","colab":{},"outputId":"97f76c5b-4b7f-4c99-f0f6-6ab4f11e0ad4"},"source":["import nltk\n","nltk.download('brown')\n","\n","from nltk.corpus import brown\n","\n","brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n","brown_news_words = brown.tagged_words(categories='news',  tagset='universal')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /Users/AlexB/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"V_SVTvqNxdq0","colab_type":"code","colab":{},"outputId":"b303b6c1-1c95-4908-d301-5b82b92e98af"},"source":["nltk.download('universal_tagset')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package universal_tagset to\n","[nltk_data]     /Users/AlexB/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"QAPZnudlxdq6","colab_type":"text"},"source":["Each sentence in the corpus as a **list of tuples**:"]},{"cell_type":"code","metadata":{"id":"KT8m8szPxdq7","colab_type":"code","colab":{},"outputId":"22837dff-1fc5-40d6-981d-56584bb51676"},"source":["type(brown_news_tagged)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["nltk.corpus.reader.util.ConcatenatedCorpusView"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"YP9bIK12xdq_","colab_type":"code","colab":{},"outputId":"1aa38b5c-d0ff-4dc3-bfbd-ff55dbc525ed"},"source":["brown_news_tagged"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"g257EVC5xdrD","colab_type":"text"},"source":["Entire corpus as a flat list of tagged words (each tagged word a tuple):"]},{"cell_type":"code","metadata":{"id":"Vxh2GkIFxdrD","colab_type":"code","colab":{},"outputId":"42ed408f-24d1-4fe3-dfda-470e7b4a6a32"},"source":["type(brown_news_words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["nltk.corpus.reader.util.ConcatenatedCorpusView"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"r3jXc0RwxdrG","colab_type":"code","colab":{},"outputId":"22cc051d-378e-4d55-e242-50a67fcc7d60"},"source":["brown_news_words"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'DET'), ('Fulton', 'NOUN'), ...]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"nFUVbWbwxdrI","colab_type":"text"},"source":["### Measuring success: Accuracy, Training Dataset, Test Dataset\n","\n","Assume we develop a tagger. How do we know how successful it is? Can we trust the decisions the tagger makes? In order to evaluate the tagger, we are going to split the dataset into training and testing:"]},{"cell_type":"code","metadata":{"id":"M-ehtgjIxdrI","colab_type":"code","colab":{},"outputId":"4f0bfd54-186d-4e5c-eb23-c44abc47ff66"},"source":["len(brown_news_tagged)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4623"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"qUar8_TpxdrK","colab_type":"text"},"source":["4623 sentences, i.e. 4623 lists of tuples, to be divided into:\n","\n","* 4523 sentences (training)\n","* 100 sentences (test)"]},{"cell_type":"code","metadata":{"id":"27u9-wCSxdrL","colab_type":"code","colab":{},"outputId":"2ba06db2-03d8-4b27-d332-45ef7ff0214e"},"source":["brown_train = brown_news_tagged[100:]\n","brown_test = brown_news_tagged[:100]\n","\n","from nltk.tag import untag\n","test_sent = untag(brown_test[0])\n","print(\"Tagged: \", brown_test[0])\n","print()\n","print(\"Untagged: \", test_sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tagged:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n","\n","Untagged:  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xEqwhxx1xdrN","colab_type":"text"},"source":["## 1. Baseline Tagger: Default Tag"]},{"cell_type":"markdown","metadata":{"id":"h4ubdIHAxdrO","colab_type":"text"},"source":["In the absence of any knowledge, the most basic tagging approach is to assign the same tag to all the words.\n","It can be done with the `DefaultTagger` class, which takes a tag and assigns it to all the words."]},{"cell_type":"code","metadata":{"id":"MtixKhBIxdrO","colab_type":"code","colab":{},"outputId":"95cef59f-86f6-4679-ceef-ee87fb96d904"},"source":["from nltk import DefaultTagger\n","\n","default_tagger = DefaultTagger('default_tag')\n","default_tagger.tag('This is a test'.split())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('This', 'default_tag'),\n"," ('is', 'default_tag'),\n"," ('a', 'default_tag'),\n"," ('test', 'default_tag')]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"eQU7X80CxdrQ","colab_type":"text"},"source":["### Exercise 1.1"]},{"cell_type":"markdown","metadata":{"id":"FgERKdL_xdrR","colab_type":"text"},"source":["Using the `DefaultTagger`, try different tags (see the available options in the table at the beginning of the notebook).\n","\n","**Which one is offering the best performance? Why?**\n","\n","To measure success, in this task, we will measure accuracy. The tagger object in NLTK includes a method called `evaluate` to measure the accuracy of a tagger on a given test set (our `brown_test` object).\n"]},{"cell_type":"markdown","metadata":{"id":"dJyrG1GExdrR","colab_type":"text"},"source":["Let's try different tags:"]},{"cell_type":"code","metadata":{"id":"_Qze2dMDxdrR","colab_type":"code","colab":{}},"source":["universal_tags = [\"ADJ\",\n","                  \"ADV\",\n","                  \"CONJ\",\n","                  \"DET\",\n","                  \"X\",\n","                  \"NOUN\",\n","                  \"PROPN\",\n","                  \"NUM\",\n","                  \"PRON\",\n","                  \"ADP\",\n","                  \"AUX\",\n","                  \"INTJ\",\n","                  \"VERB\",\n","                  \"PART\",\n","                  \"SCONJ\",\n","                  \"SYM\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAC6E8RnxdrT","colab_type":"code","colab":{}},"source":["tagger_acc = list()\n","\n","for ut in universal_tags:\n","    dt = nltk.DefaultTagger(ut)\n","    tagger_acc.append((ut, dt.evaluate(brown_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLs7moTRxdrV","colab_type":"code","colab":{},"outputId":"06ac809f-b2e7-4055-972c-c4d5cac6a23b"},"source":["sorted(tagger_acc, key=lambda x:x[1], reverse=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('NOUN', 0.31790123456790126),\n"," ('VERB', 0.1693121693121693),\n"," ('DET', 0.12301587301587301),\n"," ('ADP', 0.1128747795414462),\n"," ('ADJ', 0.05291005291005291),\n"," ('ADV', 0.025573192239858905),\n"," ('PRON', 0.021164021164021163),\n"," ('CONJ', 0.02072310405643739),\n"," ('NUM', 0.017195767195767195),\n"," ('X', 0.0),\n"," ('PROPN', 0.0),\n"," ('AUX', 0.0),\n"," ('INTJ', 0.0),\n"," ('PART', 0.0),\n"," ('SCONJ', 0.0),\n"," ('SYM', 0.0)]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"VIuEbvyMxdrW","colab_type":"code","colab":{},"outputId":"420a2bc6-b7ed-4b71-f20b-5317dfc35f7e"},"source":["tagger_acc = sorted(tagger_acc, key=lambda x:x[1], reverse=True)\n","\n","print(\"'{0}' is the tag with the best performance, \\\n","attaining an accuracy score of {1:.2f}%\".format(tagger_acc[0][0],\n","                                                tagger_acc[0][1] * 100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'NOUN' is the tag with the best performance, attaining an accuracy score of 31.79%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oU8uluzPxdrY","colab_type":"text"},"source":["The performance of each tag simply corresponds to its relative frequency in the test set:"]},{"cell_type":"code","metadata":{"id":"Cb_TuLwpxdrY","colab_type":"code","colab":{}},"source":["test_tags = [tup[1] for sentence in brown_test for tup in sentence]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_-Fvfvsxdra","colab_type":"code","colab":{},"outputId":"cb20677a-7e35-4eb2-b7ab-1c89e05cbaca"},"source":["dict(nltk.FreqDist(test_tags))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'DET': 279,\n"," 'NOUN': 721,\n"," 'ADJ': 120,\n"," 'VERB': 384,\n"," 'ADP': 256,\n"," '.': 259,\n"," 'ADV': 58,\n"," 'CONJ': 47,\n"," 'PRT': 57,\n"," 'PRON': 48,\n"," 'NUM': 39}"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"gPUsYMKMxdrb","colab_type":"code","colab":{}},"source":["test_tags_freq = dict(nltk.FreqDist(test_tags))\n","total_freq = sum(test_tags_freq.values())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTzZNoUjxdrd","colab_type":"code","colab":{}},"source":["test_tags_freq.update({k:round(test_tags_freq[k]*100/total_freq,2) for k in test_tags_freq.keys()})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hCIC4yBlxdre","colab_type":"code","colab":{},"outputId":"4c82b7c8-6ac8-44dd-abba-bb68461c030a"},"source":["test_tags_freq"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'DET': 12.3,\n"," 'NOUN': 31.79,\n"," 'ADJ': 5.29,\n"," 'VERB': 16.93,\n"," 'ADP': 11.29,\n"," '.': 11.42,\n"," 'ADV': 2.56,\n"," 'CONJ': 2.07,\n"," 'PRT': 2.51,\n"," 'PRON': 2.12,\n"," 'NUM': 1.72}"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"r97J0ImGxdrg","colab_type":"text"},"source":["Note that that the frequency of the `'NOUN'` tag corresponds to its accuracy (31.79%)."]},{"cell_type":"markdown","metadata":{"id":"hl3-fpWZxdrg","colab_type":"text"},"source":["Before moving on, let us inspect the steps implemented inside the `evaluate` method source code:\n","\n","        def evaluate(self, gold):\n","            tagged_sents = self.tag_sents(untag(sent) for sent in gold)\n","            gold_tokens = list(chain(*gold))\n","            test_tokens = list(chain(*tagged_sents))\n","            return accuracy(gold_tokens, test_tokens)\n","            \n","        def accuracy(reference, test):\n","            if len(reference) != len(test):\n","                raise ValueError(\"Lists must have the same length.\")\n","            return sum(x == y for x, y in zip(reference, test)) / len(test)\n","            \n","        class nltk.chain\n","        chain(*iterables) â€“> chain object\n","\n","        Return a chain object whose .next() method returns elements from the first iterable until it is\n","        exhausted, then elements from the next iterable, until all of the iterables are exhausted."]},{"cell_type":"markdown","metadata":{"id":"g5YQDtUrxdrh","colab_type":"text"},"source":["**Based on the above, define a function** `retrieve_incorrect_tags` **to return a list with the subset of (word,tag) tuples that have been assigned a tag different to the reference:**"]},{"cell_type":"code","metadata":{"id":"MY2YdolZxdrh","colab_type":"code","colab":{}},"source":["def retrieve_incorrect_tags(reference_tags, test_tagger):\n","    \n","    reference = list(nltk.chain(*reference_tags))\n","    test = list(nltk.chain(*test_tagger.tag_sents(untag(sent) for sent in reference_tags)))\n","    \n","    if len(reference) != len(test):\n","        raise ValueError(\"Lists must have the same length.\")\n","        \n","    reftest_wordtags = [(r,t) for r,t in zip(reference, test) if r != t]\n","    \n","    reftest_tags = [(r[1],t[1]) for r,t in reftest_wordtags]\n","    \n","    return nltk.FreqDist(reftest_tags)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YHUY-Rolxdri","colab_type":"code","colab":{},"outputId":"d0f401c7-877a-4382-ed59-5c93f8f662e1"},"source":["retrieve_incorrect_tags(brown_test, nltk.DefaultTagger('NOUN')).most_common(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('VERB', 'NOUN'), 384),\n"," (('DET', 'NOUN'), 279),\n"," (('.', 'NOUN'), 259),\n"," (('ADP', 'NOUN'), 256),\n"," (('ADJ', 'NOUN'), 120),\n"," (('ADV', 'NOUN'), 58),\n"," (('PRT', 'NOUN'), 57),\n"," (('PRON', 'NOUN'), 48),\n"," (('CONJ', 'NOUN'), 47),\n"," (('NUM', 'NOUN'), 39)]"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"markdown","metadata":{"id":"ACbD9hQMxdrk","colab_type":"text"},"source":["## 2. Sources of Knowledge to Improve Tagging Accuracy\n","\n","Intuitively, the sources of knowledge that can help us decide what is the tag of a word include:\n","- A dictionary that lists the possible parts of speech for each word\n","- The context of the word in a sentence (neighboring words)\n","- The morphological form of the word (suffixes, prefixes)\n"]},{"cell_type":"markdown","metadata":{"id":"w1ZAcWSbxdrk","colab_type":"text"},"source":["### 2.1 Lookup Tagger: Using Dictionary Knowledge\n","\n","Assume we have a dictionary that lists the possible tags for each word in English. Could we use this information to perform better tagging?\n","\n","The intuition is that we would only assign to a word a tag that it can have in the dictionary. For example, if `box` can only be a `Verb` or a `Noun`, when we have to tag an instance of the word `box`, we only choose between 2 options - and not between 17 options.\n","\n","There are 3 issues we must address to turn this into working code:\n","\n","- Where do we get the dictionary?\n","- How do we choose between the various tags associated to a word in the dictionary? (For example, how do we choose between `VERB` and `NOUN` for `box`).\n","- What do we do for words that do not appear in the dictionary?\n","\n","The simple solutions we will test are the following - note that for each question, there exist other strategies that we will investigate later:\n","\n","- Where do we get the dictionary? We will learn it from a sample dataset.\n","- How do we choose between the various tags associated to a word in the dictionary? We will choose the most likely tag as observed in the sample dataset.\n","- What do we do for words that do not appear in the dictionary? We will pass unknown words to a backoff tagger (tag all unknown words as `NOUN`).\n","\n","The `nltk.UnigramTagger` implements this overall strategy. It must be trained on a dataset, from which it builds a model of \"unigrams\". The following code shows how it is used:"]},{"cell_type":"markdown","metadata":{"id":"1z7rbuNBxdrk","colab_type":"text"},"source":["### Exercise 2.1.1\n","\n","Use the `UnigramTagger` class and the `brown_train` object to create a unigram tagger.\n","\n","**Which tag is selecting to annotate each word?**\n","\n","**What's happening with unknown words?**"]},{"cell_type":"code","metadata":{"id":"Ld9jH1jExdrl","colab_type":"code","colab":{}},"source":["from nltk import UnigramTagger"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLP4C10kxdrm","colab_type":"code","colab":{}},"source":["ungTagger = UnigramTagger(train = brown_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUL8WLZbxdro","colab_type":"code","colab":{},"outputId":"bbaf5f3e-c8ee-405e-ed3c-8782d5fc2422"},"source":["ungTagger.tag(test_sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'DET'),\n"," ('Fulton', None),\n"," ('County', 'NOUN'),\n"," ('Grand', 'ADJ'),\n"," ('Jury', 'NOUN'),\n"," ('said', 'VERB'),\n"," ('Friday', 'NOUN'),\n"," ('an', 'DET'),\n"," ('investigation', 'NOUN'),\n"," ('of', 'ADP'),\n"," (\"Atlanta's\", 'NOUN'),\n"," ('recent', 'ADJ'),\n"," ('primary', 'NOUN'),\n"," ('election', 'NOUN'),\n"," ('produced', 'VERB'),\n"," ('``', '.'),\n"," ('no', 'DET'),\n"," ('evidence', 'NOUN'),\n"," (\"''\", '.'),\n"," ('that', 'ADP'),\n"," ('any', 'DET'),\n"," ('irregularities', None),\n"," ('took', 'VERB'),\n"," ('place', 'NOUN'),\n"," ('.', '.')]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"z6CweVK9xdrp","colab_type":"text"},"source":["Each word of the training set is considered individually (constituting its own context), and in the presence of multiple tags, the most frequent one is assigned to it if encountered in the test set. Take `that` as an illustrative example:"]},{"cell_type":"code","metadata":{"id":"YSP1yUmKxdrq","colab_type":"code","colab":{}},"source":["train_tags_that = [tup[1] for sentence in brown_train for tup in sentence if tup[0].lower() == 'that']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0BllwGixdrr","colab_type":"code","colab":{},"outputId":"c62579cb-4a2c-40e6-e8e7-aba9600b4639"},"source":["nltk.FreqDist(train_tags_that)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'ADP': 520, 'DET': 150, 'PRON': 126, 'ADV': 5})"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"nhanV4TGxdrs","colab_type":"code","colab":{},"outputId":"13b8f996-3148-46f9-f532-77444569bc60"},"source":["nltk.FreqDist(train_tags_that).most_common(1)[0][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ADP'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"J6PrTiGcxdrt","colab_type":"text"},"source":["Amongst the four different POS tags assigned to `that` in the training set, `'ADP'` is the most frequent one. Hence, that is the assigned tagging when `that` is encountered in `test_sent`. Following this approach iteratively for all words in `test_sent` should retrieve the tag list from the `UnigramTagger` for that sentence:"]},{"cell_type":"code","metadata":{"id":"ty_h1Kd6xdru","colab_type":"code","colab":{}},"source":["ungTagger_list = list()\n","\n","for w in test_sent:\n","    train_tags_w = [tup[1] for sentence in brown_train for tup in sentence if tup[0] == w]\n","    if not train_tags_w:\n","        ungTagger_list.append((w,None))\n","        continue\n","    \n","    ungTagger_list.append((w, nltk.FreqDist(train_tags_w).most_common(1)[0][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mywUcwGqxdrv","colab_type":"code","colab":{},"outputId":"5438141c-ed5b-4db6-fa0c-fab5460333a4"},"source":["ungTagger_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'DET'),\n"," ('Fulton', None),\n"," ('County', 'NOUN'),\n"," ('Grand', 'ADJ'),\n"," ('Jury', 'NOUN'),\n"," ('said', 'VERB'),\n"," ('Friday', 'NOUN'),\n"," ('an', 'DET'),\n"," ('investigation', 'NOUN'),\n"," ('of', 'ADP'),\n"," (\"Atlanta's\", 'NOUN'),\n"," ('recent', 'ADJ'),\n"," ('primary', 'NOUN'),\n"," ('election', 'NOUN'),\n"," ('produced', 'VERB'),\n"," ('``', '.'),\n"," ('no', 'DET'),\n"," ('evidence', 'NOUN'),\n"," (\"''\", '.'),\n"," ('that', 'ADP'),\n"," ('any', 'DET'),\n"," ('irregularities', None),\n"," ('took', 'VERB'),\n"," ('place', 'NOUN'),\n"," ('.', '.')]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"bIvA_CcCxdrw","colab_type":"text"},"source":["Note that this list coincides with `ungTagger.tag(test_sent)`, as expected. Words that are encountered in the test set without precedents in the training set are tagged as `None`."]},{"cell_type":"markdown","metadata":{"id":"JYy9sn7Rxdrw","colab_type":"text"},"source":["### Exercise 2.1.2\n","\n","Making use of the `evaluate` method measure how successful is this tagger.\n","\n","**Are we improving the performance of the tagger?**\n","**Do your find the new performance sufficient enough for a NLP system?**"]},{"cell_type":"code","metadata":{"id":"q9I2itvrxdrx","colab_type":"code","colab":{},"outputId":"91dd091d-98d0-4488-f844-56b4c9c390e2"},"source":["round(ungTagger.evaluate(brown_test) * 100, 2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88.89"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"TsuCa9WJxdry","colab_type":"text"},"source":["There is a significant improvement compared to the baseline accuracy score, from 31.79% to 88.89%, though it is still insufficient for most NLP application purposes (1 in every 10 words is incorrectly tagged on average)."]},{"cell_type":"code","metadata":{"id":"suM6h4_wxdry","colab_type":"code","colab":{},"outputId":"7c28e355-1411-41e1-8cff-f15a95eb216e"},"source":["retrieve_incorrect_tags(brown_test, ungTagger).most_common(21)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('NOUN', None), 127),\n"," (('VERB', None), 23),\n"," (('VERB', 'NOUN'), 20),\n"," (('ADJ', None), 18),\n"," (('ADP', 'PRT'), 14),\n"," (('NOUN', 'VERB'), 10),\n"," (('NOUN', 'ADJ'), 7),\n"," (('ADV', 'ADJ'), 6),\n"," (('ADJ', 'NOUN'), 6),\n"," (('NUM', None), 5),\n"," (('ADV', None), 3),\n"," (('PRON', 'ADP'), 2),\n"," (('ADV', 'ADP'), 2),\n"," (('ADV', 'PRT'), 2),\n"," (('ADP', 'VERB'), 1),\n"," (('ADP', 'ADV'), 1),\n"," (('ADJ', 'VERB'), 1),\n"," (('ADP', None), 1),\n"," (('ADV', 'DET'), 1),\n"," (('VERB', 'ADV'), 1),\n"," (('VERB', 'ADP'), 1)]"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"O2GPh-75xdrz","colab_type":"text"},"source":["### Exercise 2.1.3\n","\n","If we analyze the tagger annotation, we will see that it assigns `None` to unknown words. A good way to improve this is to tag unknowns words as `NOUN` (the most common tag). This is known as a backoff tagger (i.e., a second tagger that applies where the original one cannot identify the tag for a word)\n","\n","NLTK provides a simple way to implement this backoff tagging. All the constructors for the Tagger classes (e.g., `UnigramTagger`) have a parameter `backoff` where you can set the backoff tagger that will apply. In this case, our backoff tagger will be the `DefaultTagger` that annotates `NOUN`, which we developed in the exercise below .\n","\n","**Using the `DefaultTagger` and `UnigramTagger` classes, create a tagger that assigns the most common tag to each word and, for unknown words, assigns a backoff tag of `NOUN`.**\n","\n","**What's the accuracy of this tagger? Do we improved our performance?**"]},{"cell_type":"code","metadata":{"id":"AkraF1F-xdr0","colab_type":"code","colab":{}},"source":["ungTaggerBckoff = UnigramTagger(train = brown_train, backoff = nltk.DefaultTagger('NOUN'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_PnpiRyxdr1","colab_type":"code","colab":{},"outputId":"36af8308-daa7-492f-ee4d-855be37c1644"},"source":["round(ungTaggerBckoff.evaluate(brown_test) * 100, 2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["94.49"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"8eqadgVBxdr2","colab_type":"text"},"source":["Accuracy improved from 88.89% to 94.49% by adding the backoff tagger."]},{"cell_type":"code","metadata":{"id":"NVygujfGxdr2","colab_type":"code","colab":{},"outputId":"ba5a494e-e7dc-4c6f-9525-66c3a09718bd"},"source":["retrieve_incorrect_tags(brown_test, ungTaggerBckoff).most_common(18)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('VERB', 'NOUN'), 43),\n"," (('ADJ', 'NOUN'), 24),\n"," (('ADP', 'PRT'), 14),\n"," (('NOUN', 'VERB'), 10),\n"," (('NOUN', 'ADJ'), 7),\n"," (('ADV', 'ADJ'), 6),\n"," (('NUM', 'NOUN'), 5),\n"," (('ADV', 'NOUN'), 3),\n"," (('PRON', 'ADP'), 2),\n"," (('ADV', 'ADP'), 2),\n"," (('ADV', 'PRT'), 2),\n"," (('ADP', 'VERB'), 1),\n"," (('ADP', 'ADV'), 1),\n"," (('ADJ', 'VERB'), 1),\n"," (('ADP', 'NOUN'), 1),\n"," (('ADV', 'DET'), 1),\n"," (('VERB', 'ADV'), 1),\n"," (('VERB', 'ADP'), 1)]"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"markdown","metadata":{"id":"Rb1P58ATxdr3","colab_type":"text"},"source":["### 2.2 Using Morphological Clues\n","\n","As mentioned above, another knowledge source to perform tagging is to look at the letter structure of the words. \n","We will look at 2 different methods to use this knowledge. \n","First, we will use `nltk.RegexpTagger` to recognize specific regular expressions in words."]},{"cell_type":"code","metadata":{"id":"Vl7UqWrWxdr4","colab_type":"code","colab":{},"outputId":"2e7f41ef-b6a0-4928-9899-02be0f92d5fc"},"source":["from nltk import RegexpTagger\n","\n","regexp_tagger = RegexpTagger(\n","     [(r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n","      (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n","      (r'.*able$', 'ADJ'),                # adjectives\n","      (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n","      (r'.*ly$', 'ADV'),                  # adverbs\n","      (r'.*s$', 'NOUN'),                  # plural nouns\n","      (r'.*ing$', 'VERB'),                # gerunds\n","      (r'.*ed$', 'VERB'),                 # past tense verbs\n","      (r'.*', 'NOUN')                     # nouns (default)\n","])\n","\n","print('Regexp accuracy %4.1f%%' % (100.0 * regexp_tagger.evaluate(brown_test)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Regexp accuracy 48.2%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xg7GV3Kqxdr5","colab_type":"text"},"source":["The regular expressions are tested in order. If one matches, it decides the tag. Else it tries the next tag. \n"]},{"cell_type":"code","metadata":{"id":"gM7r14aoxdr5","colab_type":"code","colab":{},"outputId":"ca6a62ab-c1f5-436c-cfc0-474835040b23"},"source":["retrieve_incorrect_tags(brown_test, regexp_tagger).most_common(16)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('VERB', 'NOUN'), 259),\n"," (('.', 'NOUN'), 259),\n"," (('ADP', 'NOUN'), 252),\n"," (('ADJ', 'NOUN'), 111),\n"," (('DET', 'NOUN'), 63),\n"," (('PRT', 'NOUN'), 57),\n"," (('PRON', 'NOUN'), 48),\n"," (('CONJ', 'NOUN'), 47),\n"," (('ADV', 'NOUN'), 45),\n"," (('NUM', 'NOUN'), 16),\n"," (('NOUN', 'VERB'), 6),\n"," (('ADP', 'VERB'), 4),\n"," (('ADJ', 'ADV'), 3),\n"," (('ADJ', 'VERB'), 2),\n"," (('VERB', 'ADJ'), 1),\n"," (('NOUN', 'ADV'), 1)]"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"KhAltGObxdr7","colab_type":"text"},"source":["The question we face when we see such a \"rule-based\" tagger are:\n","\n","- How do we find the most successful regular expressions?\n","- In which order should we try the regular expressions?\n","\n","A typical answer to such questions is: \n","\n","- let's learn these parameters from a training corpus. \n","\n","The `nltk.AffixTagger` is a trainable tagger that attempts to learn word patterns. \n","It only looks at the last letters in the words in the training corpus, and counts how often a word suffix \n","can predict the word tag. \n","In other words, we only learn rules of the form ('.*xyz' , POS). \n","This is how the affix tagger is used:"]},{"cell_type":"code","metadata":{"id":"5C8Qtb3qxdr7","colab_type":"code","colab":{},"outputId":"1f1b30d7-1967-4298-b038-5e5de654db02"},"source":["from nltk import AffixTagger\n","\n","affix_tagger = AffixTagger(brown_train, backoff=DefaultTagger('NOUN'))\n","print('Affix tagger accuracy: %4.2f%%' % (100.0 * affix_tagger.evaluate(brown_test)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Affix tagger accuracy: 42.28%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-COivaUixdr8","colab_type":"code","colab":{},"outputId":"74dc1e71-026b-419b-9ecf-edc9e3b9cb50"},"source":["retrieve_incorrect_tags(brown_test, affix_tagger).most_common(26)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('DET', 'NOUN'), 260),\n"," (('.', 'NOUN'), 259),\n"," (('ADP', 'NOUN'), 239),\n"," (('VERB', 'NOUN'), 221),\n"," (('ADJ', 'NOUN'), 48),\n"," (('PRT', 'NOUN'), 48),\n"," (('CONJ', 'NOUN'), 47),\n"," (('PRON', 'NOUN'), 45),\n"," (('NUM', 'NOUN'), 38),\n"," (('ADV', 'NOUN'), 35),\n"," (('NOUN', 'ADJ'), 20),\n"," (('NOUN', 'VERB'), 10),\n"," (('NOUN', 'ADV'), 6),\n"," (('ADJ', 'ADV'), 5),\n"," (('ADP', 'ADJ'), 5),\n"," (('ADP', 'VERB'), 4),\n"," (('ADV', 'ADJ'), 3),\n"," (('ADJ', 'VERB'), 3),\n"," (('ADV', 'PRT'), 3),\n"," (('ADV', 'VERB'), 2),\n"," (('VERB', 'ADJ'), 2),\n"," (('VERB', 'ADP'), 2),\n"," (('NOUN', 'ADP'), 1),\n"," (('NOUN', 'DET'), 1),\n"," (('ADJ', 'ADP'), 1),\n"," (('ADV', 'ADP'), 1)]"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"markdown","metadata":{"id":"3_zXzdUuxdr-","colab_type":"text"},"source":["Should we be disappointed that the \"data-based approach\" performs worse than the hand-written rules (42% vs. 48%)? \n","\n","Not necessarily: note that our hand-written rules include cases that the AffixTagger cannot learn - we match cardinal numbers and suffixes with more than 3 letters. \n","\n","Let us see whether the combination of the 2 taggers helps."]},{"cell_type":"markdown","metadata":{"id":"hBsBrdk3xdr-","colab_type":"text"},"source":["### Exercise 2.2.1\n","\n","**Using the `AffixTagger` class, create a tagger that learns from word patterns and that uses the previous `RegexpTagger` as backoff.**\n","\n","**Evaluate and analyze the performance of the model**"]},{"cell_type":"code","metadata":{"id":"3oR1im7Axdr-","colab_type":"code","colab":{}},"source":["afTaggerBckoff = AffixTagger(brown_train, backoff=regexp_tagger)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAIonhgtxdr_","colab_type":"code","colab":{},"outputId":"ad42c8bc-9939-490f-bb19-aa6b845a9c7b"},"source":["round(afTaggerBckoff.evaluate(brown_test) * 100, 2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["52.87"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"ji9vvJaMxdsA","colab_type":"text"},"source":["The accuracy score of the `AffixTagger` improves about 10% by setting the `RegexpTagger` as backoff instead of `DefaultTagger('NOUN')`. Note that the regular expression tagger still resorts to `'NOUN'` as default tag when no pattern match is found."]},{"cell_type":"code","metadata":{"id":"nFQFZMMmxdsA","colab_type":"code","colab":{},"outputId":"361f3f50-65e8-4c1f-8375-c263b204d6bf"},"source":["retrieve_incorrect_tags(brown_test, afTaggerBckoff).most_common(26)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('.', 'NOUN'), 259),\n"," (('ADP', 'NOUN'), 239),\n"," (('VERB', 'NOUN'), 221),\n"," (('ADJ', 'NOUN'), 48),\n"," (('PRT', 'NOUN'), 48),\n"," (('CONJ', 'NOUN'), 47),\n"," (('PRON', 'NOUN'), 45),\n"," (('DET', 'NOUN'), 44),\n"," (('ADV', 'NOUN'), 34),\n"," (('NOUN', 'ADJ'), 20),\n"," (('NUM', 'NOUN'), 15),\n"," (('NOUN', 'VERB'), 10),\n"," (('NOUN', 'ADV'), 6),\n"," (('ADJ', 'ADV'), 5),\n"," (('ADP', 'ADJ'), 5),\n"," (('ADP', 'VERB'), 4),\n"," (('ADV', 'ADJ'), 3),\n"," (('ADJ', 'VERB'), 3),\n"," (('ADV', 'PRT'), 3),\n"," (('ADV', 'VERB'), 2),\n"," (('VERB', 'ADJ'), 2),\n"," (('VERB', 'ADP'), 2),\n"," (('NOUN', 'ADP'), 1),\n"," (('NOUN', 'DET'), 1),\n"," (('ADJ', 'ADP'), 1),\n"," (('ADV', 'ADP'), 1)]"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"markdown","metadata":{"id":"gHRF5Br5xdsB","colab_type":"text"},"source":["### Exercise 2.2.2\n","\n","In the previous exercise we created an `AffixTagger` that is able to learn the annotation from the word patterns. Perhaps, we could apply this tagger to annotate the unknown words (i.e., to use it as a backoff tagger). In the previous section, we used a NOUN-default tagger for that. How much does this tagger help the `UnigramTagger` if we use it as a backoff instead of the NOUN-default tagger?\n","\n","**Use the `AffixTagger` that we created below as a backoff tagger for the `UnigramTagger` in the previous section**\n","\n","**Are we improving our tagger?**"]},{"cell_type":"code","metadata":{"id":"lZUZbtguxdsC","colab_type":"code","colab":{}},"source":["ungTaggerBckoff2 = UnigramTagger(train = brown_train, backoff = afTaggerBckoff)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aM3VogXxdsD","colab_type":"code","colab":{},"outputId":"5f3360ec-714f-4dca-e06a-ac05fa39bf86"},"source":["round(ungTaggerBckoff2.evaluate(brown_test) * 100, 2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["95.41"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"QTFY5N0vxdsE","colab_type":"text"},"source":["By changing the backoff tagger from `DefaultTagger('NOUN')` to `AffixTagger`, the accuracy score of the `UnigramTagger` improves by about 1%."]},{"cell_type":"code","metadata":{"id":"sM-88fc5xdsE","colab_type":"code","colab":{},"outputId":"24447092-4622-40a0-e304-95b16a232fbc"},"source":["retrieve_incorrect_tags(brown_test, ungTaggerBckoff2).most_common(18)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('VERB', 'NOUN'), 27),\n"," (('NOUN', 'VERB'), 14),\n"," (('ADP', 'PRT'), 14),\n"," (('ADJ', 'NOUN'), 11),\n"," (('NOUN', 'ADJ'), 10),\n"," (('ADV', 'ADJ'), 6),\n"," (('NOUN', 'ADV'), 4),\n"," (('VERB', 'ADP'), 3),\n"," (('ADP', 'VERB'), 2),\n"," (('ADJ', 'VERB'), 2),\n"," (('PRON', 'ADP'), 2),\n"," (('ADV', 'ADP'), 2),\n"," (('ADV', 'PRT'), 2),\n"," (('ADP', 'ADV'), 1),\n"," (('VERB', 'ADJ'), 1),\n"," (('ADV', 'NOUN'), 1),\n"," (('ADV', 'DET'), 1),\n"," (('VERB', 'ADV'), 1)]"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"SLolihItxdsF","colab_type":"text"},"source":["###  2.3 Looking at the Context\n","\n","At this point, we have combined 2 major sources of information: dictionary and morphology and obtained about 95.4% accuracy. The last source of knowledge we want to exploit is the context of the word to be tagged: **the words that appear around the word to be tagged**. \n","\n","The intuition is that if we have to decide between `book` as a verb or a noun, the word/s preceding `book` can give us strong cues: for example, if it is an article (`the` or `a`) then we would be sure that `book` is a noun; if it is `to`, then we would be sure it is a verb.\n","\n","How can we turn this intuition into working code? The easiest way to detect predictive contexts is to construct a list of contexts - and for each context, keep track of the distribution of tags that follow it. Luckily for us, this procedure is already implemented into the `NgramTagger`, which takes as parameter a number setting the length of the context.\n","\n","As usual, if the tagger cannot make a decision (because the observed context was never seen at training time), \n","the decision is delegated to a backoff tagger."]},{"cell_type":"markdown","metadata":{"id":"byuY42j_xdsF","colab_type":"text"},"source":["### Exercise 2.3.1\n","\n","**Use the `NgramTagger` to create a context-based tagger. For the cases that this tagger cannot annotate anything, use the previous `UnigramTagger` as backoff.**\n","\n","**Try different context sizes (you can set that as a parameter when you create the `NgramTagger`) and analyze how it affects to the final performance of the model.**"]},{"cell_type":"code","metadata":{"id":"4ALZQ-AcxdsF","colab_type":"code","colab":{}},"source":["from nltk import NgramTagger"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qk3JKviJxdsG","colab_type":"text"},"source":["Initially try with a bigram tagger as `NgramTagger(n=2)`:"]},{"cell_type":"code","metadata":{"id":"4V64KwVIxdsG","colab_type":"code","colab":{}},"source":["n2Tagger = NgramTagger(2, brown_train, backoff=ungTaggerBckoff2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nEgHsl6xdsH","colab_type":"code","colab":{},"outputId":"18b31867-c568-41f6-c93e-ae522efe0058"},"source":["round(n2Tagger.evaluate(brown_test) * 100, 2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["96.12"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"xyPvmbbPxdsI","colab_type":"code","colab":{},"outputId":"ed8f9071-d2a5-4681-99a7-7c6f7a2be9e6"},"source":["retrieve_incorrect_tags(brown_test, n2Tagger).most_common(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('VERB', 'NOUN'), 15),\n"," (('NOUN', 'VERB'), 12),\n"," (('ADP', 'PRT'), 12),\n"," (('ADJ', 'NOUN'), 11),\n"," (('NOUN', 'ADJ'), 7),\n"," (('NOUN', 'ADV'), 4),\n"," (('ADV', 'ADJ'), 4),\n"," (('PRT', 'ADP'), 3),\n"," (('VERB', 'ADP'), 3),\n"," (('ADP', 'VERB'), 2),\n"," (('ADJ', 'VERB'), 2),\n"," (('ADP', 'ADV'), 2),\n"," (('PRON', 'ADP'), 2),\n"," (('ADV', 'ADP'), 2),\n"," (('ADV', 'PRT'), 2),\n"," (('ADJ', 'X'), 1),\n"," (('VERB', 'ADJ'), 1),\n"," (('ADV', 'NOUN'), 1),\n"," (('ADJ', 'ADV'), 1),\n"," (('ADV', 'DET'), 1)]"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"markdown","metadata":{"id":"Jba9cyRwxdsJ","colab_type":"text"},"source":["Looping over a range of values (2, 3, 4, 5, 6) for the size of the n-grams:"]},{"cell_type":"code","metadata":{"id":"WFg8kFWjxdsJ","colab_type":"code","colab":{}},"source":["nTagger_acc_list = list()\n","\n","for n in range(2,7):\n","    nTagger = NgramTagger(n, brown_train, backoff=ungTaggerBckoff2)\n","    nTagger_acc_list.append((n, round(nTagger.evaluate(brown_test) * 100, 2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xj1ImmL_xdsK","colab_type":"code","colab":{},"outputId":"325881a6-98b6-4f33-964d-2317e4a54a2b"},"source":["nTagger_acc_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(2, 96.12), (3, 95.81), (4, 95.19), (5, 94.89), (6, 94.93)]"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"5PloDBKVxdsL","colab_type":"text"},"source":["The accuracy score of the `NgramTagger` drops with the size of the n-grams, to the point that the 4-gram tagger performs worse than the `UnigramTagger` set as backoff. This is expected, since larger n-grams provide more information but are also more unlikely to be found in the training set."]},{"cell_type":"markdown","metadata":{"id":"Oc4PQrq3xdsL","colab_type":"text"},"source":["## Summary\n","\n","This practice introduced tools to tag parts of speech in free text. The key point of the approach we investigated is that it is **data-driven**:\n","\n","- We first define possible knowledge sources that can help us solve the task. Specifically, we investigated \n","  * dictionary, \n","  * morphological \n","  * context\n","  as possible sources.\n","\n","- We tested simple machine learning methods: data is acquired by inspecting a training dataset, then evaluated by testing on a test dataset.\n","\n","- We investigated one method to combine several systems into a combined system: backoff models.\n"]},{"cell_type":"markdown","metadata":{"id":"CxV051b-xdsL","colab_type":"text"},"source":["# Additional Materials: Practical Tagging\n","\n","In this practice we have played with the development of new Taggers. You can refer back to this Notebook if and when you need to create your own Taggers. Nevertheless, most of the time the Taggers already included in the different libraries will do the trick for you.\n","\n","In particular, NLTK provides you a way to tag your dataset with just a couple of lines of code by using the `pos_tag` function."]},{"cell_type":"markdown","metadata":{"id":"gd5HviWyxdsM","colab_type":"text"},"source":["The first thing we need to do is to tokenize the sentence to be tagged. To that end, we can make use of the `word_tokenize` function in NLTK."]},{"cell_type":"code","metadata":{"id":"3i1Ccp_9xdsM","colab_type":"code","colab":{},"outputId":"5b1fe29f-edd0-4efe-fb88-9f4386613c59"},"source":["text = nltk.word_tokenize(\"And now for something completely different\")\n","text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['And', 'now', 'for', 'something', 'completely', 'different']"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"EPr2TfsCxdsN","colab_type":"text"},"source":["Then we should feed our tokenized text to the pos tagging function"]},{"cell_type":"code","metadata":{"id":"AvT9Wx65xdsN","colab_type":"code","colab":{},"outputId":"1b53bf07-0276-42fd-df9a-273bcbcd5742"},"source":["nltk.pos_tag(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('And', 'CC'),\n"," ('now', 'RB'),\n"," ('for', 'IN'),\n"," ('something', 'NN'),\n"," ('completely', 'RB'),\n"," ('different', 'JJ')]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"IUbIClK7xdsO","colab_type":"text"},"source":["If we have more than one sentence to parse, we can make use of some of the Sentence Tokenizers that nltk provides (e.g. `sent_tokenize`) to split the text in sentences, and the the word tokenizer to split each sentence in words."]},{"cell_type":"code","metadata":{"id":"yQ9O4vQnxdsO","colab_type":"code","colab":{},"outputId":"95e9c5b0-dcca-4477-ee52-0f538d1550d2"},"source":["sentences = nltk.sent_tokenize(\"And now for something completely different. This is just another sentence\")\n","print(\"Sentences:\", sentences)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentences: ['And now for something completely different.', 'This is just another sentence']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gzNaF3xZxdsQ","colab_type":"code","colab":{},"outputId":"12accb63-3733-4788-8919-cf735fd801a5"},"source":["text = [nltk.word_tokenize(sentence) for sentence in sentences]\n","print(\"Text:\", text)\n","print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text: [['And', 'now', 'for', 'something', 'completely', 'different', '.'], ['This', 'is', 'just', 'another', 'sentence']]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IcK-dCfmxdsR","colab_type":"code","colab":{},"outputId":"bf9858f6-e3ab-4fe6-daab-1912d3696f32"},"source":["for tagging in [nltk.pos_tag(t) for t in text]:\n","    print(\"Tagging:\",tagging)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tagging: [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ'), ('.', '.')]\n","Tagging: [('This', 'DT'), ('is', 'VBZ'), ('just', 'RB'), ('another', 'DT'), ('sentence', 'NN')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3LWXBfHFxdsS","colab_type":"text"},"source":["Let's see a full example with a proper corpus. NLTK provides many corpora that can be used for research or for the training of our NLP system. To find a comprehensive list of all the corpus and how to use them, please refer to the [2nd Chapter of the NLTK book](https://www.nltk.org/book/ch02.html).\n","\n","We will use the corpus `state_union` including the texts of the State of the Union addresses since 1945. Let's load one of these speeches."]},{"cell_type":"code","metadata":{"id":"fJlmA1pzxdsS","colab_type":"code","colab":{},"outputId":"ef22093d-5e52-4170-f459-995a93cf1c8d"},"source":["from nltk.corpus import state_union\n","\n","text = state_union.raw(\"1946-Truman.txt\")\n","text[:1000]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946.\\n \\nJanuary 21, 1946. Dated January 14, 1946 \\n\\nTo the Congress of the United States:\\nA quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. Instead it has called on the President to present a comprehensive Executive Budget. The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. The bigger and more complex the Federal Program, the more necessary it is for the Chief Executive to submit a single budget for action by the Congress.\\nAt the same time, it is clear that the budgetary program and the general program of the Government are actually inseparable. The President bears the responsibility for recommending to the Congress a comprehensive set of proposals on all Government activities and their financing. In formulating policies, as in\""]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"dnWFtvEpxdsT","colab_type":"text"},"source":["We now define a function `tag_corpus` that takes care of the tagging process. First, it splits the text in sentences with the `sent_tokenize` function. Then, it iterates over these sentences, tokenize them with the `word_tokenize` function and apply the `pos_tag` function to the tokens."]},{"cell_type":"code","metadata":{"id":"i3ZEjQP-xdsT","colab_type":"code","colab":{}},"source":["def tag_corpus(corpus_text, end):\n","    try:\n","        for sentence in nltk.sent_tokenize(corpus_text)[:end]:\n","            words = nltk.word_tokenize(sentence)\n","            tagged = nltk.pos_tag(words)\n","            print(\"Sentence:\", sentence, \"\\nTagging:\", tagged)\n","            print()\n","\n","    except Exception as e:\n","        print(str(e))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQQv9E9BxdsU","colab_type":"code","colab":{},"outputId":"fa224393-5e76-49a7-9b9c-5666286151db"},"source":["tag_corpus(text, end=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence: PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946. \n","Tagging: [('PRESIDENT', 'NNP'), ('HARRY', 'NNP'), ('S.', 'NNP'), ('TRUMAN', 'NNP'), (\"'S\", 'POS'), ('MESSAGE', 'NN'), ('TO', 'VBD'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('AND', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('BUDGET', 'NNP'), ('FOR', 'NNP'), ('1946', 'CD'), ('.', '.')]\n","\n","Sentence: January 21, 1946. \n","Tagging: [('January', 'NNP'), ('21', 'CD'), (',', ','), ('1946', 'CD'), ('.', '.')]\n","\n","Sentence: Dated January 14, 1946 \n","\n","To the Congress of the United States:\n","A quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. \n","Tagging: [('Dated', 'VBN'), ('January', 'NNP'), ('14', 'CD'), (',', ','), ('1946', 'CD'), ('To', 'TO'), ('the', 'DT'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (':', ':'), ('A', 'DT'), ('quarter', 'NN'), ('century', 'NN'), ('ago', 'IN'), ('the', 'DT'), ('Congress', 'NNP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('could', 'MD'), ('no', 'RB'), ('longer', 'RB'), ('consider', 'VB'), ('the', 'DT'), ('financial', 'JJ'), ('programs', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('various', 'JJ'), ('departments', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('piecemeal', 'JJ'), ('basis', 'NN'), ('.', '.')]\n","\n","Sentence: Instead it has called on the President to present a comprehensive Executive Budget. \n","Tagging: [('Instead', 'RB'), ('it', 'PRP'), ('has', 'VBZ'), ('called', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('President', 'NNP'), ('to', 'TO'), ('present', 'VB'), ('a', 'DT'), ('comprehensive', 'JJ'), ('Executive', 'NNP'), ('Budget', 'NNP'), ('.', '.')]\n","\n","Sentence: The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. \n","Tagging: [('The', 'DT'), ('Congress', 'NNP'), ('has', 'VBZ'), ('shown', 'VBN'), ('its', 'PRP$'), ('satisfaction', 'NN'), ('with', 'IN'), ('that', 'DT'), ('method', 'NN'), ('by', 'IN'), ('extending', 'VBG'), ('the', 'DT'), ('budget', 'NN'), ('system', 'NN'), ('and', 'CC'), ('tightening', 'VBG'), ('its', 'PRP$'), ('controls', 'NNS'), ('.', '.')]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zN9BximgxdsV","colab_type":"text"},"source":["Same function in a more *pythonic* way"]},{"cell_type":"code","metadata":{"id":"K-CM4srmxdsW","colab_type":"code","colab":{}},"source":["def pythonized_tag_corpus(corpus_text, end):\n","    try:\n","        [print(\"Sentence:\", sentence, \"\\nTagging:\", nltk.pos_tag(nltk.word_tokenize(sentence)), \"\\n\") for sentence in nltk.sent_tokenize(corpus_text)[:end]]\n","    \n","    except Exception as e:\n","        print(str(e))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"iClisE4_xdsX","colab_type":"code","colab":{},"outputId":"cdf92a83-73e7-4068-980a-b51d969818d6"},"source":["pythonized_tag_corpus(text, end=5)    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence: PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946. \n","Tagging: [('PRESIDENT', 'NNP'), ('HARRY', 'NNP'), ('S.', 'NNP'), ('TRUMAN', 'NNP'), (\"'S\", 'POS'), ('MESSAGE', 'NN'), ('TO', 'VBD'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('AND', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('BUDGET', 'NNP'), ('FOR', 'NNP'), ('1946', 'CD'), ('.', '.')] \n","\n","Sentence: January 21, 1946. \n","Tagging: [('January', 'NNP'), ('21', 'CD'), (',', ','), ('1946', 'CD'), ('.', '.')] \n","\n","Sentence: Dated January 14, 1946 \n","\n","To the Congress of the United States:\n","A quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. \n","Tagging: [('Dated', 'VBN'), ('January', 'NNP'), ('14', 'CD'), (',', ','), ('1946', 'CD'), ('To', 'TO'), ('the', 'DT'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (':', ':'), ('A', 'DT'), ('quarter', 'NN'), ('century', 'NN'), ('ago', 'IN'), ('the', 'DT'), ('Congress', 'NNP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('could', 'MD'), ('no', 'RB'), ('longer', 'RB'), ('consider', 'VB'), ('the', 'DT'), ('financial', 'JJ'), ('programs', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('various', 'JJ'), ('departments', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('piecemeal', 'JJ'), ('basis', 'NN'), ('.', '.')] \n","\n","Sentence: Instead it has called on the President to present a comprehensive Executive Budget. \n","Tagging: [('Instead', 'RB'), ('it', 'PRP'), ('has', 'VBZ'), ('called', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('President', 'NNP'), ('to', 'TO'), ('present', 'VB'), ('a', 'DT'), ('comprehensive', 'JJ'), ('Executive', 'NNP'), ('Budget', 'NNP'), ('.', '.')] \n","\n","Sentence: The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. \n","Tagging: [('The', 'DT'), ('Congress', 'NNP'), ('has', 'VBZ'), ('shown', 'VBN'), ('its', 'PRP$'), ('satisfaction', 'NN'), ('with', 'IN'), ('that', 'DT'), ('method', 'NN'), ('by', 'IN'), ('extending', 'VBG'), ('the', 'DT'), ('budget', 'NN'), ('system', 'NN'), ('and', 'CC'), ('tightening', 'VBG'), ('its', 'PRP$'), ('controls', 'NNS'), ('.', '.')] \n","\n"],"name":"stdout"}]}]}